<!DOCTYPE html>

<meta charset=utf-8>
<title>blog.izs.me</title>
<link rel=stylesheet href=../backup.css>

<body class=post>

<header>
</header>
<article class=quote id=p-9839460782>
<header>
<p><time datetime=2011-09-05T17:22:32Z>09/05/2011 10:22:32</time>
<a class=llink href=../posts/9839460782.html>¶</a>
<a href=https://tmblr.co/Z7nwWy9AUa6k>●</a></header>
<blockquote><p>In the universe where everything works the way it common-sensically ought to, everything about the study of Artificial General Intelligence is driven by the one overwhelming fact of the indescribably huge effects: initial conditions and unfolding patterns whose consequences will resound for as long as causal chains continue out of Earth, until all the stars and galaxies in the night sky have burned down to cold iron, and maybe long afterward, or forever into infinity if the true laws of physics should happen to permit that.  To deliberately thrust your mortal brain onto that stage, as it plays out on ancient Earth the first root of life, is an act so far beyond &ldquo;audacity&rdquo; as to set the word on fire, an act which can only be excused by the terrifying knowledge that the empty skies offer no higher authority.</p></blockquote>
<p>Eliezer Yudkowsky, <a href="http://lesswrong.com/lw/uc/aboveaverage_ai_scientists/">Above-Average AI Scientists</a></p>
<footer>83 notes</footer>
</article>
